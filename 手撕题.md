# 手撕机器学习

## 逻辑回归

```python
import numpy as np
def sigmoid(x):
    return 1/(1+np.exp(-x))

def logistic(X, y, W, b):
    batch_size = X.shape[0]
    hidden_size = X.shape[1]

    predict = sigmoid(np.dot(X, W) + b) # y hat
    loss = -1*(y*np.log(predict) - (1-y)*np.log(1-predict))/batch_size

    dW = np.dot(X, predict-y)/batch_size
    db = np.sum(predict-y)/batch_size

    return predict, loss, dW, db

def train():    
    for i in range(epochs):
        p, loss, dW, db = logistic(X, y, W, b)
        W -= lr*dW
        b -= lr*db
```

## K-Means

```python
import numpy as np
import matplotlib.pyplot as plt

n = 10 ** 3  # num_sample
d = 2  # num_dim
k = 3  # num_cluster

x = np.random.randn(n, d) / 3  # data samples
y = np.random.randint(k, size=[n, 1])  # asigned clusters
mu = np.random.randn(k, d)  # cluster_centroids

convergence = False
while not convergence:
    y_old = np.copy(y)
    # E-step: Assign each observation to the cluster with the nearest mean.
    for i in range(n):
        sq_dist = np.sum((x[i] - mu) ** 2, axis=1)  # square_distance
        y[i, 0] = np.argmin(sq_dist)
    # M-step: Recalculate means (centroids) for observations assigned to each cluster.
    for i in range(k):
        mask = (y == i)
        mu[i] = np.sum(x * mask, axis=0) / np.sum(mask)
    convergence = all(y == y_old)

plt.scatter(x[:, 0], x[:, 1], c=y, cmap='cool')
plt.scatter(
    mu[:, 0], mu[:, 1],
    c='white',
    edgecolors='black',
    linewidths=3
)
plt.show()
```

## 决策树

## 朴素贝叶斯

# 手撕深度学习

## 手撕self-attention

```python
# MHA手撕版本
def attention(q, k, v):
    head = 8
    dim_per_head = hidden_size // head
 
    def head_shape(x):
        return x.reshape(batch_size, -1, head, dim_per_head).transpose(1, 2)

    def head_unshape(x):
        return x.transpose(1, 2).contiguous().reshape(batch_size, -1, head * dim_per_head)

    q = head_shape(q)
    k = head_shape(k)
    v = head_shape(v)

    query = linear_querys(q)
    key = linear_keys(k)

    query /= math.sqrt(dim_per_head)
    attn = torch.matual(q, k.transpose(2, 3))
    attn = softmax(attn)

    res = torch.matual(attn, v)
    res = head_unshape(res)

    return res
```

## 手撕MLP



# 手撕排序算法

## 快排

```python
def quick(arr, start, end):
    if start>=end:
        return
    
    i, j = start, end
    # 这里我们选择第一个元素作为pivot
    # 随机选择可以降低复杂度
    pivot = arr[start]
    while i<j:
        while i<j and arr[j]>=pivot:
            j-=1
        arr[i] = arr[j]
        while i<j and arr[i]<pivot:
            i+=1
        arr[j] = arr[i]
       
    arr[i] = pivot
    quick(arr, start, i-1)
    quick(arr, i+1, end)
```

## 堆排

```python
def topDown(arr, i, size):
    l = i*2+1 # 左节点索引
    r = i*2+2 # 右节点索引
    max_idx = i
    if l<size and arr[l]>arr[max_idx]:
        max_idx = l
    if r<size and arr[r]>=arr[max_idx]:
        max_idx = r
    if i!=max_idx:
        arr[i], arr[max_idx] = arr[max_idx], arr[i]
        topDown(arr, max_idx, size) # 递归 topDown


def heapSort(arr):
    for i in range(len(arr)//2-1, -1, -1): # 从最后一个有子节点的节点开始topDown
        topDown(arr, i, len(arr))
    # 此时已经构建了一个最大堆，堆顶为该数组最大值
    for i in range(len(arr)-1, -1, -1):
        arr[0], arr[i] = arr[i], arr[0] # 将最大值交换到数组尾部
        topDown(arr, 0, i) # 保持堆的特性，做一次topDown
```

## 归并排序

```python
def mergeSort(arr, l, r):
    if l>=r: return
    m = (l+r)//2
    mergeSort(arr, l, m)
    mergeSort(arr, m+1, r)
    tmp[l:r+1] = arr[l:r+1]
    i, j = l, m+1
    for k in range(l, r+1):
        if i==m+1:
            arr[k] = tmp[j]
            j+=1
        elif j==r+1:
            arr[k] = tmp[i]
            i+=1
        elif tmp[i]<tmp[j]:
            arr[k] = tmp[i]
            i+=1
        else:
            arr[k] = tmp[j]
            j+=1  
  
arr = [2, 5, 5, 6, 3, 90, 2]
n = len(arr)
tmp = [0]*n
mergeSort(arr, 0, n-1)
```

